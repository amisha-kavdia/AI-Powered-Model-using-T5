# -*- coding: utf-8 -*-
"""Grammer_Error.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_f1MSJybw2tkzalCQGDfsnK12yRI7QYZ
"""

!pip install transformers
!pip install datasets
!pip install torch
!pip install scikit-learn

import json

with open("Dataset.json", "r", encoding="utf-8") as file:
    data = json.load(file)

t5_train_data = []
for item in data:
  Incorrect_sentence = item["Incorrect sentence"]
  Correct_sentence = item["Correct sentence"]
  Explanation = item["Explanation"]

  input_text = f"fix: {item['Incorrect sentence']}"
  target_text = f"{item['Correct sentence']} | Explanation:{'Explanation'}"
  t5_train_data.append((input_text, target_text))


print(t5_train_data[:3])

from transformers import T5Tokenizer

tokenizer = T5Tokenizer.from_pretrained("t5-base")


tokenized_data = [
    {
        "input_ids": tokenizer.encode(inp, max_length=128, padding="max_length", truncation=True),
        "labels": tokenizer.encode(tgt, max_length=128, padding="max_length", truncation=True)
    }
    for inp, tgt in t5_train_data
]


print(tokenized_data[:2])

import torch
from torch.utils.data import Dataset, DataLoader

class GrammarCorrectionDataset(Dataset):
    def __init__(self, tokenized_data):
        self.tokenized_data = tokenized_data

    def __len__(self):
        return len(self.tokenized_data)

    def __getitem__(self, idx):
        item = self.tokenized_data[idx]
        return {
            "input_ids": torch.tensor(item["input_ids"], dtype=torch.long),
            "labels": torch.tensor(item["labels"], dtype=torch.long)
        }

dataset = GrammarCorrectionDataset(tokenized_data)


print(f"Dataset size: {len(dataset)}")

batch_size = 10
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)


for batch in dataloader:
    print(batch["input_ids"].shape, batch["labels"].shape)
    break

from transformers import T5ForConditionalGeneration


model = T5ForConditionalGeneration.from_pretrained("t5-base")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print(f"Using device: {device}")

from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)

epochs = 15

from tqdm import tqdm

def train(model, dataloader, optimizer, epochs):
    model.train()

    for epoch in range(epochs):
        print(f"\nüöÄ Training Epoch {epoch+1}/{epochs}")
        loop = tqdm(dataloader, leave=True)

        for batch in loop:
            optimizer.zero_grad()


            input_ids = batch["input_ids"].to(device)
            labels = batch["labels"].to(device)


            outputs = model(input_ids=input_ids, labels=labels)
            loss = outputs.loss


            loss.backward()
            optimizer.step()


            loop.set_description(f"Epoch {epoch+1}")
            loop.set_postfix(loss=loss.item())

    print("\n‚úÖ Training Complete!")

train(model, dataloader, optimizer, epochs)

def correct_grammar(sentence):
    model.eval()


    input_text = f"fix: {sentence}"
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)

    with torch.no_grad():
        output_ids = model.generate(input_ids, max_length=128, num_beams=5, early_stopping=True)


    corrected_sentence = tokenizer.decode(output_ids[0], skip_special_tokens=True)


    return corrected_sentence.strip()

test_sentences = [
    "She go to school every day.",
    "He dont like pizza.",
    "I has a pen.",
    "They is meeting today.",
    "She said that she can‚Äôt comes.",
    "She is afraid from dogs.",
    "He like to dance.",
    "She is the most funniest of all.",
    "She is going to abroad next week.",
    "The baby has been crying since four hours.",
    "She is fond with painting landscapes.",
    "She is looking forward to meet her cousins.",
    "She is senior than her manager.",
    "She told to me that she was leaving.",
    "We were very much happy to see them."]


for sentence in test_sentences:
    corrected = correct_grammar(sentence)
    print(f"‚ùå Incorrect: {sentence}")
    print(f"‚úÖ Corrected: {corrected}")

import nltk
from nltk.translate.bleu_score import sentence_bleu


def compute_bleu(reference, hypothesis):
    reference_tokens = [reference.split()]
    hypothesis_tokens = hypothesis.split()
    return sentence_bleu(reference_tokens, hypothesis_tokens)

reference_sentence = "She goes to school every day."
generated_sentence = "She goes to school every day."

bleu_score = compute_bleu(reference_sentence, generated_sentence)
print(f"BLEU Score: {bleu_score:.4f}")

test_sentences = [
    {"Incorrect": "She go to school every day.", "correct": "She goes to school every day."},
    {"Incorrect": "He don't like pizza.", "correct": "He doesn't like pizza."},
    {"Incorrect": "I has a pen.", "correct": "I have a pen."},
    {"Incorrect": "They is meeting today.", "correct": "They are meeting today."},
    {"Incorrect": "She said that she can‚Äôt comes.", "correct": "She said that she can't come."}
]

total_bleu= 0
for test in test_sentences:
    generated = correct_grammar(test["Incorrect"])
    bleu = compute_bleu(test["correct"], generated)
    total_bleu += bleu

    print(f"‚ùå Incorrect: {test['Incorrect']}")
    print(f"‚úÖ Corrected: {generated}")
    print(f"üìä BLEU: {bleu:.4f}")


avg_bleu = total_bleu / len(test_sentences)
print(f"\nüöÄ Average BLEU Score: {avg_bleu:.4f}")